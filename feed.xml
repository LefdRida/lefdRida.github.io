<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lefdrida.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lefdrida.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-23T20:35:02+00:00</updated><id>https://lefdrida.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">RNN and Hierarchical Attention Network</title><link href="https://lefdrida.github.io/posts/2025/HAN_blog/" rel="alternate" type="text/html" title="RNN and Hierarchical Attention Network"/><published>2025-01-20T15:12:00+00:00</published><updated>2025-01-20T15:12:00+00:00</updated><id>https://lefdrida.github.io/posts/2025/HAN_blog</id><content type="html" xml:base="https://lefdrida.github.io/posts/2025/HAN_blog/"><![CDATA[<p>In this posts, we will try to get familiar with RNN, self-attention and HAN architectures. Data exists in many types such as tabular, imgaes, graph, texts etc. Sequential data is data arranged in an ordered sequence. It could be ordered by time, e.g time series, or by position, e.g text. generally, we model the sequence as follows: \((x_{1}, x_{2}, …, x_{T})\), where \((x_{i})\) could be a word in case of a text, a real value in case of a time series etc… .</p> <p>Recurrent Neural Network are very suitable to work with sequential data and make prediction using this kind of data. In this posts, we will Talk about RNNs and some of its variants, GRU and BiGRU and HAN architecture which uses self attention with BidirGRU. Also, we will see an application of HAN to classify IMDB movie reviews. Other interesting kind of RNN such as LSTM will be covered later in another post.</p> <h3 id="recurrent-neural-network---rnn">Recurrent Neural Network - RNN</h3> <p>An RNN takes as input a sequence \((x_{1}, x_{2}, …, x_{T})\) and outputs a sequence of hidden states (or representations) \((h_{1}, h_{2}, …, h_{T})\) which are often called <em>annnotations</em>. At each time step \(t\), the RNN unit takes as input \(x_{t}\) and the previous hidden state \(h_{t-1}\) to compute the hidden state as follows:</p> \[h_{t} = \sigma (U x_{t} + W h_{t-1} + b)\] <p>The hidden representations are used then to make a prediction. This makes RNNs relays only on past information and no future information are used:</p> \[y_{t} = \sigma (W_{y} h_{t} + b')\] <p>RNNs can process input of any size and the model size remains independent of the input size as the weight are share across time. But they are computationally slow and they have difficulties to learn long term dependancies. So, they suffers from vanishing/exploding gradient because of multiplicative gradient that can be exponentially decreasing or increasing with respect to the number of layers.</p> <h3 id="gated-recurrent-unit---gru">Gated Recurrent Unit - GRU</h3> <p>GRU are an RNN that deals with vanishing gradient problem through using specific gates. In GRU we have two gates, reset (relevance) gate and update gate, defined as follows:</p> <p>\begin{equation}r_{t} = \sigma (U_{r} x_{t} + W_{r} h_{t-1} + b_{r})\end{equation}</p> <p>\begin{equation}z_{t} = \sigma (U_{z} x_{t} + W_{z} h_{t-1} + b_{z})\end{equation}</p> <p>The reset gate determines how much information should be discarder from previous time steps stored in \(h_{t-1}\).</p> <p>So we compute a candidate hidden state using this reset gates as follows :</p> \[\hat{h}_t = tanh(U_{h} x_{t} + W_{h}(r_{t} \circ h_{t-1}) + b_{h})\] <p>This candidates hidden states is used along with the previous hidden states to obtain the final hidden state by linearly interpolating them using the update gates:</p> \[h_t= (1 - z_{t}) \circ h_{t-1} + z_{t} \circ \hat{h}_t\] <h3 id="bidirectional-gru">Bidirectional GRU</h3> <p>Bidirectional GRU is GRU variant that consider hidden state from previous and future steps to predict the current hidden state:</p> \[h_t^{+}= gru(x_{t}; h_{t+1})\] \[h_t^{-}= gru(x_{t}; h_{t-1})\] \[h_t= h_t^{-} \circ h_t^{+}\] <p>Bidirectional GRU can be implemented easily using <em>Pytorch</em></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GRU</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="c1"># The size of the input
</span>            <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span> <span class="c1"># The size of the hidden state 
</span>            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></div> <p>The <em>bidirectional</em> argument is set to True to specify a biderctional GRU. If it is set to False then we have normal GRU</p> <h3 id="self-attention">Self Attention</h3> <p>The attention mechanism was developed in encoder-decoder architecture for NMT context and then used for other context. The attention also used in encoder only settings and it is called self or inner attention. Self attention is the core component of transformers.</p> <p>The idea behind attention is to use for prediction a weighted sum, where all the weights are determined using trainable parameters, of the all hidden states \((h_{1}, h_{2}, …, h_{T})\) rather than using the last hidden state which is prone to information loss.</p> <p>The hidden states are passed to a dense layer (eq 3). The alignment coefficient are computed by comparing the output of the dense layer with a trainable context vector u and normalized using Softmax (eq 4). The attentional vector is computed then using a weighted sum of hidden states with alignment coefficient as weights (eq 5).</p> <p>\begin{equation}u_t = tanh(W h_t) \end{equation} \begin{equation}\alpha_t = \frac{exp(u_{t}^{T}u)}{\sum_{t’=1}^{T}exp(u_{t’}^{T}u)} \end{equation} \begin{equation}s = \sum_{t=1}^{T}\alpha_{t}h_{t}\end{equation}</p> <p>The self attention can be implemented as follows using Pytorch</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>


<span class="k">class</span> <span class="nc">AttentionWithContext</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Class implementing self attention mechanism:
    u_t = tanh(W . x_t)
    a_t = (u_t^T . u)/sum(u_i^T . u)
    s = sum(a_t * x_t)
    </span><span class="sh">"""</span>
  
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">return_coefficients</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">AttentionWithContext</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">return_coefficients</span> <span class="o">=</span> <span class="n">return_coefficients</span>

        <span class="c1"># Dense Layer W
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>
        <span class="c1"># Trainable context vector u 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="nf">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">initrange</span> <span class="o">=</span> <span class="mf">0.1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">initrange</span><span class="p">,</span> <span class="n">initrange</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">initrange</span><span class="p">,</span> <span class="n">initrange</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">u</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">initrange</span><span class="p">,</span> <span class="n">initrange</span><span class="p">)</span>

   
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
        <span class="c1"># compute uit = tanh(W . h)  where h are the hidden states
</span>        <span class="n">uit</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>  
        <span class="n">uit</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">uit</span><span class="p">)</span> <span class="c1">#(N, L, d) --&gt; (N, L, d)
</span>        
        <span class="c1"># compute the attention coefficient alphas : u_t^T . u
</span>        <span class="n">ait</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">u</span><span class="p">(</span><span class="n">uit</span><span class="p">)</span> <span class="c1">#(N, L, 1) --&gt; (N, L, 1)
</span>        <span class="c1">#Normalizing with softmax
</span>        <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ait</span><span class="p">)</span>
        <span class="c1"># in some cases especially in the early stages of training the sum may be almost zero
</span>        <span class="c1"># and this results in NaN's. A workaround is to add a very small positive number ε to the sum.
</span>        <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-9</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="c1">#(N, L, 1) --&gt; (N, L, 1)
</span>        
        <span class="c1"># Compute the attentional vector : s = sum(a_t * x_t)
</span>        <span class="n">weighted_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">return_coefficients</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span>
                <span class="n">weighted_input</span><span class="p">,</span>
                <span class="n">a</span><span class="p">,</span>
            <span class="p">)</span>  <span class="c1">### [attentional vector, coefficients] ### use torch.sum to compute s
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weighted_input</span>  <span class="c1">### attentional vector only ###
</span></code></pre></div></div> <p>The attention mechanism can provide similar summation weights for all the hidden states. A penalization term was proposed by [9] to encourage the diversity of summation weight vectors. The penalization term is as follows:</p> \[P = {\lVert (AA^{T} - I) \rVert}_{F}^{2}\] <p>where F refer to the frobenius norm of a matrix \({\lVert A \rVert}_{F}^{2} = \sum_{i=1}^{n}\sum_{j=1}^{n} a_{ij}^{2}\)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">Frobenius</span><span class="p">(</span><span class="n">mat</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">mat</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># batched matrix
</span>        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">mat</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-10</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="o">/</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">Exception</span><span class="p">(</span><span class="sh">'</span><span class="s">matrix for computing Frobenius norm should be with 3 dims</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Create a batch of indentity matrices of size L where L represent tha length of the sequence.
</span><span class="n">I</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">L</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">I</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1">#Computing the penalization term
</span><span class="n">A_T</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
<span class="n">P</span> <span class="o">=</span> <span class="nc">Frobenius</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A_T</span><span class="p">)</span> <span class="o">-</span> <span class="n">I</span><span class="p">[:</span><span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span> 
</code></pre></div></div> <h3 id="hierarchical-attention-network---han">Hierarchical Attention Network - HAN</h3> <p>Hierarchical Attention Network is an interesting architecture which use self attention and was proposed by [5]. The architecture contains many level and each level is RNNs followed by self attention layer. Which makes this architecture suitable for data that has a hierarchy e.g word —&gt; sentence —&gt; document. First, a sentence encoder produces an embedding for each sentence from word embeddings; then a document encoder produces the document embedding vector from the sentence embeddings previously produced. Each encoder is a Bidir GRU followed by a self attention.</p> <p>HAN makes sense for two reason: First it matches the natural hierarchy of a document; second; it allows the model to first determine which words are important in each sentence and then which sentence are important overall.</p> <p>By being able to re-weight the word attentional coefficients by the sentence attentional coefficients the model captures the fact that a word may be very important in a sentence but it’s less important in another sentence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AttentionBiGRU</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">AttentionBiGRU</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span>
            <span class="nf">len</span><span class="p">(</span><span class="n">index_to_word</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="c1"># vocab size
</span>            <span class="n">d</span><span class="p">,</span>  <span class="c1"># dimensionality of embedding space
</span>            <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GRU</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">d</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
            <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">AttentionWithContext</span><span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">n_units</span><span class="p">,</span>  <span class="c1"># the input shape for the attention layer
</span>            <span class="n">return_coefficients</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sent_ints</span><span class="p">):</span>
        <span class="n">sent_wv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">sent_ints</span><span class="p">)</span>
        <span class="n">sent_wv_dr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">sent_wv</span><span class="p">)</span>
        <span class="n">sent_wa</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gru</span><span class="p">(</span><span class="n">sent_wv_dr</span><span class="p">)</span>  <span class="c1"># GRU layer
</span>        <span class="n">sent_att_vec</span><span class="p">,</span> <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span>
            <span class="n">sent_wa</span>
        <span class="p">)</span>  <span class="c1"># attentional vector for the sent
</span>        <span class="n">sent_att_vec_dr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">sent_att_vec</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sent_att_vec_dr</span><span class="p">,</span> <span class="n">word_att_coeffs</span>


<span class="k">class</span> <span class="nc">TimeDistributed</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="n">self</span><span class="p">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Squash samples and timesteps into a single axis
</span>        <span class="n">x_reshape</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>  <span class="c1"># (samples * timesteps, input_size) (224, 30)
</span>        <span class="n">sent_att_vec_dr</span><span class="p">,</span> <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">module</span><span class="p">(</span><span class="n">x_reshape</span><span class="p">)</span>
        <span class="c1"># We have to reshape the output
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_first</span><span class="p">:</span>
            <span class="n">sent_att_vec_dr</span> <span class="o">=</span> <span class="n">sent_att_vec_dr</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sent_att_vec_dr</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># (samples, timesteps, output_size)
</span>            <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">word_att_coeffs</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">word_att_coeffs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># (samples, timesteps, output_size)
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">sent_att_vec_dr</span> <span class="o">=</span> <span class="n">sent_att_vec_dr</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sent_att_vec_dr</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># (timesteps, samples, output_size)
</span>            <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">word_att_coeffs</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">word_att_coeffs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># (timesteps, samples, output_size)
</span>        <span class="k">return</span> <span class="n">sent_att_vec_dr</span><span class="p">,</span> <span class="n">word_att_coeffs</span>


<span class="k">class</span> <span class="nc">HAN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">HAN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">AttentionBiGRU</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">timeDistributed</span> <span class="o">=</span> <span class="nc">TimeDistributed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GRU</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_units</span><span class="p">,</span>  <span class="c1"># the input shape of GRU layer
</span>            <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">AttentionWithContext</span><span class="p">(</span>
            <span class="mi">2</span>
            <span class="o">*</span> <span class="n">n_units</span><span class="p">,</span>  <span class="c1"># the input shape of between-sentence attention layer
</span>            <span class="n">return_coefficients</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lin_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">n_units</span><span class="p">,</span> <span class="mi">1</span>  <span class="c1"># the input size of the last linear layer
</span>        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">preds</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">doc_ints</span><span class="p">):</span>
        <span class="n">sent_att_vecs_dr</span><span class="p">,</span> <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">timeDistributed</span><span class="p">(</span>
            <span class="n">doc_ints</span>
        <span class="p">)</span>  <span class="c1"># get sentence representation
</span>        <span class="n">doc_sa</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gru</span><span class="p">(</span><span class="n">sent_att_vecs_dr</span><span class="p">)</span>
        <span class="n">doc_att_vec</span><span class="p">,</span> <span class="n">sent_att_coeffs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">doc_sa</span><span class="p">)</span>
        <span class="n">doc_att_vec_dr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">doc_att_vec</span><span class="p">)</span>
        <span class="n">doc_att_vec_dr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lin_out</span><span class="p">(</span><span class="n">doc_att_vec_dr</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">preds</span><span class="p">(</span><span class="n">doc_att_vec_dr</span><span class="p">),</span> <span class="n">word_att_coeffs</span><span class="p">,</span> <span class="n">sent_att_coeffs</span>

</code></pre></div></div> <p>While HAN is an interesting architecture, it has a major limitation. For a given sentence, the embedding is produced in isolation. This means, it ignores the other sentences. So, If redundant parts exist in the document; the model can spend the attention budget on them and neglects the others aspects.</p> <h3 id="dataset">Dataset</h3> <p>The dataset considered in this application is IMDB review dataset. It contains reviews about movies and their classes, positive or negative. The goal is to classify these reviews into positive or negative.</p> <p>Each review is converted to an array of integers that has a size of \((1, doc_size, sent_size)\). the <em>doc{_}size</em> specifies the maximum number of allowed sentences per document and <em>sent{_}size</em> the specifies the maximum number of allowed words per sentence. Smaller sentences are padded by a special padding token and smaller documents are padded with sentences containing only a special padding token. Longer documents or sentences are truncated.</p> <p>The mapping of a word to an integers is done by creating a vocabulary dictionary from the training set where each word has an integer value. The most frequent word has a value of 2. 0 and 1 are reserved for special token and out of vocabulary token.</p> <p>An example of a sentence and its mapping to an array of integers:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sentence:

"There 's a sign on The Lost Highway that says : OOV SPOILERS OOV ( but you already knew that , did n't you ? )"
</code></pre></div></div> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Corresponding array:

array([  130,    14,     6,  1991,    28,    22,  2746, 17943,    13,
         564,    85,     1,  3225,     1,    25,    26,    29,   488,
         697,    13,     3,    84,    27,    29,    45,    24,     0,
           0,     0,     0])
</code></pre></div></div> <p>We have some 0 in the end of the array as the choosen <em>sent_size</em> is 30 and the sentence needs to be padded because it contains only 26 words.</p> <h3 id="training">Training</h3> <p>The training consists of minimizing a loss function. As we are dealing with classification problem, the loss used is <em>Binary Cross Entropy (BCE)</em>:</p> \[L(\hat{y}, y) =\frac{1}{N} \sum_{i=1}^N y_i log(\hat{y}_i) + (1-y_i)log(1-\hat{y}_i)\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">HAN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># learning rate
</span><span class="n">criterion</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BCELoss</span><span class="p">()</span>
<span class="p">)</span>  <span class="c1"># Binary cross entropy from torch.nn: https://pytorch.org/docs/stable/nn.html#loss-functions
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>  <span class="c1"># Adam optimizer
</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="n">x_train</span><span class="o">=</span><span class="n">my_docs_array_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="o">=</span><span class="n">my_labels_array_train</span><span class="p">,</span>
    <span class="n">x_test</span><span class="o">=</span><span class="n">my_docs_array_test</span><span class="p">,</span>
    <span class="n">y_test</span><span class="o">=</span><span class="n">my_labels_array_test</span><span class="p">,</span>
    <span class="n">word_dict</span><span class="o">=</span><span class="n">index_to_word</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="p">):</span>

    <span class="n">train_data</span> <span class="o">=</span> <span class="nf">get_loader</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="nf">get_loader</span><span class="p">(</span><span class="n">my_docs_array_test</span><span class="p">,</span> <span class="n">my_labels_array_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="n">best_validation_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># patience
</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">with</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="sh">"</span><span class="s">batch</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">tepoch</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">tepoch</span><span class="p">):</span>
                <span class="n">tepoch</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="nb">input</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">document</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>    <span class="c1"># compute the loss
</span>                <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">0.5</span>
                <span class="p">)</span>  <span class="c1"># Clipping to prevent exploding gradient
</span>                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

                <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
                <span class="n">accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="n">label</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">batch_size</span>
                <span class="n">accuracies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
                <span class="n">tepoch</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span>
                    <span class="n">loss</span><span class="o">=</span><span class="nf">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span>
                    <span class="n">accuracy</span><span class="o">=</span><span class="mf">100.0</span> <span class="o">*</span> <span class="nf">sum</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">accuracies</span><span class="p">),</span>
                <span class="p">)</span>

        <span class="n">train_acc</span> <span class="o">=</span> <span class="nf">evaluate_accuracy</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="nf">evaluate_accuracy</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">===&gt; Epoch {} Complete: Avg. Loss: {:.4f}, Validation Accuracy: {:3.2f}%</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">test_acc</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">test_acc</span> <span class="o">&gt;=</span> <span class="n">best_validation_acc</span><span class="p">:</span>
            <span class="n">best_validation_acc</span> <span class="o">=</span> <span class="n">test_acc</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Validation accuracy improved, saving model...</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sh">"</span><span class="s">./best_model.pt</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nf">print</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="n">my_patience</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span>
                    <span class="sh">"</span><span class="s">Validation accuracy did not improve for {} epochs, stopping training...</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                        <span class="n">my_patience</span>
                    <span class="p">)</span>
                <span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading best checkpoint...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">./best_model.pt</span><span class="sh">"</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">done.</span><span class="sh">"</span><span class="p">)</span>


<span class="nf">train</span><span class="p">()</span>
</code></pre></div></div> <h3 id="analysis">Analysis</h3> <p>Using the mean of the hidden state of GRU instead attention will lead to treating the hidden state in a sequence equally. This lead to the same contribution of irrelevant elements to the prediction with the same importance as relevant element. Which can be subject to poor performances. So, the advantage of the attention mechanism is to have weights releaving the importance of each hidden state; and so each element, in the sequence (words in case of sentence, and sentences in case of a document).</p> <p>The following plots, shows the attention coefficient per words and per sentences of positive and negative reviews from the test set. The more the sentence or the word is red, the more the attention of this element is high. <em>&lt;|OOV|&gt;</em> means Out Of Vocabulary, is a token to replace words that are not in the training vocabulary.</p> <p>Example of positive review :</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/han_blog/last_review_coeff-480.webp 480w,/assets/blog/han_blog/last_review_coeff-800.webp 800w,/assets/blog/han_blog/last_review_coeff-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/han_blog/last_review_coeff.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Example of Negative review :</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/han_blog/neg_review_coeff-480.webp 480w,/assets/blog/han_blog/neg_review_coeff-800.webp 800w,/assets/blog/han_blog/neg_review_coeff-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/han_blog/neg_review_coeff.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>From the plots, we can see that words and sentence that relevant and more linked to a positive or negative reviews have a high attention coefficient regarding the other words. For example in the positive review, we have words such as “Brilliant”, “Master Piece”, “Great” which are positive and have a high attention. Same remark for sentences such as “:) First of all, Mulholland Drive is downright brilliant.” or “A masterpiece”. In the negative review we have words or sentences with a negative tonality have a high attention; such as “confused” “suspenful”.</p>]]></content><author><name></name></author><category term="Technical-post"/><category term="RNN"/><category term="Self-Attention"/><category term="HAN"/><summary type="html"><![CDATA[In this posts, we will try to get familiar with RNN, self-attention and HAN architectures. Data exists in many types such as tabular, imgaes, graph, texts etc. Sequential data is data arranged in an ordered sequence. It could be ordered by time, e.g time series, or by position, e.g text. generally, we model the sequence as follows: \((x_{1}, x_{2}, …, x_{T})\), where \((x_{i})\) could be a word in case of a text, a real value in case of a time series etc… .]]></summary></entry><entry><title type="html">Energy Forecasting with LSTM</title><link href="https://lefdrida.github.io/posts/2025/LSTM_times_series_pred/" rel="alternate" type="text/html" title="Energy Forecasting with LSTM"/><published>2025-01-20T15:12:00+00:00</published><updated>2025-01-20T15:12:00+00:00</updated><id>https://lefdrida.github.io/posts/2025/LSTM_times_series_pred</id><content type="html" xml:base="https://lefdrida.github.io/posts/2025/LSTM_times_series_pred/"><![CDATA[<p>In this post we will cover LSTM through time series forecasting. The aim is to explain LSTM and its advantage regarding RNNs. We will cover an application of Energy consumption and cover also some statistical algorithm for comparison.</p> <p>As we explained previously RNN takes as input a sequence of elements and outputs a sequece of hidden states. Each unit in RNN take as input an element from sequence and the hidden state of the previous element from the sequence. Dealing with sequences makes RNN suitable for making prediction using time series as data. Time series is a sequence of elements, that could be real value, images or texts, that is ordered in a chronological order.</p> <p>A first problem of RNN is its inability to perform well on tasks that require the use of information distant from the current point of procssing. The hidden states tends to be local and relevant only the most recent parts of the input sequence. The second problem is vanishing gradients, as during training the error needs to backpropagate through time and then we have repeated multiplications which results in that gradients are eventualy driven to zero.</p> <p>To address this limitations, more complex network was designed. And LSTM is the most commonly used as an extension to RNN.</p> <h3 id="long-short-term-memory--lstm">Long Short Term Memory : LSTM</h3> <p>Long Short term memory solves the limitation of neglicting information distant from the current time step, encountered in RNN. It achieves this by adding a context layer to the architecture that output a context vector \(c_t\). And, adding, also, gates that allow either to remove information that are no longer needed or to add information to be needed for later decision making.</p> <p>The first gate to consider is the <em>forget gate</em> \(f_t\). Its purpose is to delete information that are no longer needed and computed as follows:</p> \[f_t = \sigma (U_f h_{t-1} + W_fx_t)\] \[k_t = c_t \odot f_t\] <p>The second gate to consider is the <em>add gate</em> which aims to select information to add to the current context and computed as follows:</p> \[i_t = \sigma (U_i h_{t-1} + W_i x_t)\] \[j_t = g_t \odot i_t\] <p>where \(g_t\) is the actual information we need to extract from the previous hidden state and current input :</p> \[g_t = tanh (U_g h_{t-1} + W_g x_t)\] <p>the output of the add gate and the forget gate are summed then to get the current context vector:</p> \[c_t = j_t + k_t\] <p>finally we have the output gate that decides what information is required for the current hidden state. Is is computed as follows:</p> \[o_t = \sigma (U_o h_{t-1} + W_o x_t)\] \[h_t = o_t \odot tanh(c_t)\] <p>We can remark that the gates have a similar design. They contain a feedforward layer, followed by a sigmoid function, and finally followed by an element-wise multiplication with the layer being gated.<br/> The choice of <em>sigmoid</em> function arises from its tendency to push its output to either 0 or 1. With the use of the element-wise multiplication, we have the effect of a binary mask. This allows then the values in the layer being gated that align with values near to 1 to pass and the values that align vaues near 0 to be erased. This makes the intuition behind how the add gate or the forget gate select or delete information.</p> <p>The LSTM can be implemented using Pytorch simply as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="c1"># The size of the input
</span>            <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span> <span class="c1"># The size of the hidden state 
</span>            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></div> <p>to be continued …</p>]]></content><author><name></name></author><category term="Technical-post"/><summary type="html"><![CDATA[In this post we will cover LSTM through time series forecasting. The aim is to explain LSTM and its advantage regarding RNNs. We will cover an application of Energy consumption and cover also some statistical algorithm for comparison.]]></summary></entry><entry><title type="html">Hello World</title><link href="https://lefdrida.github.io/posts/2025/hello_world/" rel="alternate" type="text/html" title="Hello World"/><published>2025-01-20T15:12:00+00:00</published><updated>2025-01-20T15:12:00+00:00</updated><id>https://lefdrida.github.io/posts/2025/hello_world</id><content type="html" xml:base="https://lefdrida.github.io/posts/2025/hello_world/"><![CDATA[<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/usr/bin/env python
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Technical-post"/><summary type="html"><![CDATA[#!/usr/bin/env python print("Hello World")]]></summary></entry></feed>