<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RNN and Hierarchical Attention Network | Rida Lefdali </title> <meta name="author" content="Rida Lefdali"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lefdrida.github.io/posts/2025/HAN_blog/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rida</span> Lefdali </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/posts/index.html">posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">RNN and Hierarchical Attention Network</h1> <p class="post-meta"> Created on January 20, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> RNN   <i class="fa-solid fa-hashtag fa-sm"></i> Self-Attention   <i class="fa-solid fa-hashtag fa-sm"></i> HAN   ·   <i class="fa-solid fa-tag fa-sm"></i> Technical-post </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this posts, we will try to get familiar with RNN, self-attention and HAN architectures. Data exists in many types such as tabular, imgaes, graph, texts etc. Sequential data is data arranged in an ordered sequence. It could be ordered by time, e.g time series, or by position, e.g text. generally, we model the sequence as follows: \((x_{1}, x_{2}, …, x_{T})\), where \((x_{i})\) could be a word in case of a text, a real value in case of a time series etc… .</p> <p>Recurrent Neural Network are very suitable to work with sequential data and make prediction using this kind of data. In this posts, we will Talk about RNNs and some of its variants, GRU and BiGRU and HAN architecture which uses self attention with BidirGRU. Also, we will see an application of HAN to classify IMDB movie reviews. Other interesting kind of RNN such as LSTM will be covered later in another post.</p> <h3 id="recurrent-neural-network---rnn">Recurrent Neural Network - RNN</h3> <p>An RNN takes as input a sequence \((x_{1}, x_{2}, …, x_{T})\) and outputs a sequence of hidden states (or representations) \((h_{1}, h_{2}, …, h_{T})\) which are often called <em>annnotations</em>. At each time step \(t\), the RNN unit takes as input \(x_{t}\) and the previous hidden state \(h_{t-1}\) to compute the hidden state as follows:</p> \[h_{t} = \sigma (U x_{t} + W h_{t-1} + b)\] <p>The hidden representations are used then to make a prediction. This makes RNNs relays only on past information and no future information are used:</p> \[y_{t} = \sigma (W_{y} h_{t} + b')\] <p>RNNs can process input of any size and the model size remains independent of the input size as the weight are share across time. But they are computationally slow and they have difficulties to learn long term dependancies. So, they suffers from vanishing/exploding gradient because of multiplicative gradient that can be exponentially decreasing or increasing with respect to the number of layers.</p> <h3 id="gated-recurrent-unit---gru">Gated Recurrent Unit - GRU</h3> <p>GRU are an RNN that deals with vanishing gradient problem through using specific gates. In GRU we have two gates, reset (relevance) gate and update gate, defined as follows:</p> <p>\begin{equation}r_{t} = \sigma (U_{r} x_{t} + W_{r} h_{t-1} + b_{r})\end{equation}</p> <p>\begin{equation}z_{t} = \sigma (U_{z} x_{t} + W_{z} h_{t-1} + b_{z})\end{equation}</p> <p>The reset gate determines how much information should be discarder from previous time steps stored in \(h_{t-1}\).</p> <p>So we compute a candidate hidden state using this reset gates as follows :</p> \[\hat{h}_t = tanh(U_{h} x_{t} + W_{h}(r_{t} \circ h_{t-1}) + b_{h})\] <p>This candidates hidden states is used along with the previous hidden states to obtain the final hidden state by linearly interpolating them using the update gates:</p> \[h_t= (1 - z_{t}) \circ h_{t-1} + z_{t} \circ \hat{h}_t\] <h3 id="bidirectional-gru">Bidirectional GRU</h3> <p>Bidirectional GRU is GRU variant that consider hidden state from previous and future steps to predict the current hidden state:</p> \[h_t^{+}= gru(x_{t}; h_{t+1})\] \[h_t^{-}= gru(x_{t}; h_{t-1})\] \[h_t= h_t^{-} \circ h_t^{+}\] <p>Bidirectional GRU can be implemented easily using <em>Pytorch</em></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GRU</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="c1"># The size of the input
</span>            <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span> <span class="c1"># The size of the hidden state 
</span>            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></div> <p>The <em>bidirectional</em> argument is set to True to specify a biderctional GRU. If it is set to False then we have normal GRU</p> <h3 id="self-attention">Self Attention</h3> <p>The attention mechanism was developed in encoder-decoder architecture for NMT context and then used for other context. The attention also used in encoder only settings and it is called self or inner attention. Self attention is the core component of transformers.</p> <p>The idea behind attention is to use for prediction a weighted sum, where all the weights are determined using trainable parameters, of the all hidden states \((h_{1}, h_{2}, …, h_{T})\) rather than using the last hidden state which is prone to information loss.</p> <p>The hidden states are passed to a dense layer (eq 3). The alignment coefficient are computed by comparing the output of the dense layer with a trainable context vector u and normalized using Softmax (eq 4). The attentional vector is computed then using a weighted sum of hidden states with alignment coefficient as weights (eq 5).</p> <p>\begin{equation}u_t = tanh(W h_t) \end{equation} \begin{equation}\alpha_t = \frac{exp(u_{t}^{T}u)}{\sum_{t’=1}^{T}exp(u_{t’}^{T}u)} \end{equation} \begin{equation}s = \sum_{t=1}^{T}\alpha_{t}h_{t}\end{equation}</p> <p>The self attention can be implemented as follows using Pytorch</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>


<span class="k">class</span> <span class="nc">AttentionWithContext</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Class implementing self attention mechanism:
    u_t = tanh(W . x_t)
    a_t = (u_t^T . u)/sum(u_i^T . u)
    s = sum(a_t * x_t)
    </span><span class="sh">"""</span>
  
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">return_coefficients</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">AttentionWithContext</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">return_coefficients</span> <span class="o">=</span> <span class="n">return_coefficients</span>

        <span class="c1"># Dense Layer W
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>
        <span class="c1"># Trainable context vector u 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="nf">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">initrange</span> <span class="o">=</span> <span class="mf">0.1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">initrange</span><span class="p">,</span> <span class="n">initrange</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">initrange</span><span class="p">,</span> <span class="n">initrange</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">u</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">initrange</span><span class="p">,</span> <span class="n">initrange</span><span class="p">)</span>

   
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
        <span class="c1"># compute uit = tanh(W . h)  where h are the hidden states
</span>        <span class="n">uit</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>  
        <span class="n">uit</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">uit</span><span class="p">)</span> <span class="c1">#(N, L, d) --&gt; (N, L, d)
</span>        
        <span class="c1"># compute the attention coefficient alphas : u_t^T . u
</span>        <span class="n">ait</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">u</span><span class="p">(</span><span class="n">uit</span><span class="p">)</span> <span class="c1">#(N, L, 1) --&gt; (N, L, 1)
</span>        <span class="c1">#Normalizing with softmax
</span>        <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ait</span><span class="p">)</span>
        <span class="c1"># in some cases especially in the early stages of training the sum may be almost zero
</span>        <span class="c1"># and this results in NaN's. A workaround is to add a very small positive number ε to the sum.
</span>        <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-9</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="c1">#(N, L, 1) --&gt; (N, L, 1)
</span>        
        <span class="c1"># Compute the attentional vector : s = sum(a_t * x_t)
</span>        <span class="n">weighted_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">return_coefficients</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span>
                <span class="n">weighted_input</span><span class="p">,</span>
                <span class="n">a</span><span class="p">,</span>
            <span class="p">)</span>  <span class="c1">### [attentional vector, coefficients] ### use torch.sum to compute s
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weighted_input</span>  <span class="c1">### attentional vector only ###
</span></code></pre></div></div> <p>The attention mechanism can provide similar summation weights for all the hidden states. A penalization term was proposed by [9] to encourage the diversity of summation weight vectors. The penalization term is as follows:</p> \[P = {\lVert (AA^{T} - I) \rVert}_{F}^{2}\] <p>where F refer to the frobenius norm of a matrix \({\lVert A \rVert}_{F}^{2} = \sum_{i=1}^{n}\sum_{j=1}^{n} a_{i}_{j}^{2}\)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">Frobenius</span><span class="p">(</span><span class="n">mat</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">mat</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># batched matrix
</span>        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">mat</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-10</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="o">/</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">Exception</span><span class="p">(</span><span class="sh">'</span><span class="s">matrix for computing Frobenius norm should be with 3 dims</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Create a batch of indentity matrices of size L where L represent tha length of the sequence.
</span><span class="n">I</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">L</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">I</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1">#Computing the penalization term
</span><span class="n">A_T</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
<span class="n">P</span> <span class="o">=</span> <span class="nc">Frobenius</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A_T</span><span class="p">)</span> <span class="o">-</span> <span class="n">I</span><span class="p">[:</span><span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span> 
</code></pre></div></div> <h3 id="hierarchical-attention-network---han">Hierarchical Attention Network - HAN</h3> <p>Hierarchical Attention Network is an interesting architecture which use self attention and was proposed by [5]. The architecture contains many level and each level is RNNs followed by self attention layer. Which makes this architecture suitable for data that has a hierarchy e.g word —&gt; sentence —&gt; document. First, a sentence encoder produces an embedding for each sentence from word embeddings; then a document encoder produces the document embedding vector from the sentence embeddings previously produced. Each encoder is a Bidir GRU followed by a self attention.</p> <p>HAN makes sense for two reason: First it matches the natural hierarchy of a document; second; it allows the model to first determine which words are important in each sentence and then which sentence are important overall.</p> <p>By being able to re-weight the word attentional coefficients by the sentence attentional coefficients the model captures the fact that a word may be very important in a sentence but it’s less important in another sentence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AttentionBiGRU</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">AttentionBiGRU</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span>
            <span class="nf">len</span><span class="p">(</span><span class="n">index_to_word</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="c1"># vocab size
</span>            <span class="n">d</span><span class="p">,</span>  <span class="c1"># dimensionality of embedding space
</span>            <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GRU</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">d</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
            <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">AttentionWithContext</span><span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">n_units</span><span class="p">,</span>  <span class="c1"># the input shape for the attention layer
</span>            <span class="n">return_coefficients</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sent_ints</span><span class="p">):</span>
        <span class="n">sent_wv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">sent_ints</span><span class="p">)</span>
        <span class="n">sent_wv_dr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">sent_wv</span><span class="p">)</span>
        <span class="n">sent_wa</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gru</span><span class="p">(</span><span class="n">sent_wv_dr</span><span class="p">)</span>  <span class="c1"># GRU layer
</span>        <span class="n">sent_att_vec</span><span class="p">,</span> <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span>
            <span class="n">sent_wa</span>
        <span class="p">)</span>  <span class="c1"># attentional vector for the sent
</span>        <span class="n">sent_att_vec_dr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">sent_att_vec</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sent_att_vec_dr</span><span class="p">,</span> <span class="n">word_att_coeffs</span>


<span class="k">class</span> <span class="nc">TimeDistributed</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="n">self</span><span class="p">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Squash samples and timesteps into a single axis
</span>        <span class="n">x_reshape</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>  <span class="c1"># (samples * timesteps, input_size) (224, 30)
</span>        <span class="n">sent_att_vec_dr</span><span class="p">,</span> <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">module</span><span class="p">(</span><span class="n">x_reshape</span><span class="p">)</span>
        <span class="c1"># We have to reshape the output
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_first</span><span class="p">:</span>
            <span class="n">sent_att_vec_dr</span> <span class="o">=</span> <span class="n">sent_att_vec_dr</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sent_att_vec_dr</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># (samples, timesteps, output_size)
</span>            <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">word_att_coeffs</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span>
                <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">word_att_coeffs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># (samples, timesteps, output_size)
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">sent_att_vec_dr</span> <span class="o">=</span> <span class="n">sent_att_vec_dr</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sent_att_vec_dr</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># (timesteps, samples, output_size)
</span>            <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">word_att_coeffs</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">word_att_coeffs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># (timesteps, samples, output_size)
</span>        <span class="k">return</span> <span class="n">sent_att_vec_dr</span><span class="p">,</span> <span class="n">word_att_coeffs</span>


<span class="k">class</span> <span class="nc">HAN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">HAN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">AttentionBiGRU</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">timeDistributed</span> <span class="o">=</span> <span class="nc">TimeDistributed</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GRU</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_units</span><span class="p">,</span>  <span class="c1"># the input shape of GRU layer
</span>            <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">AttentionWithContext</span><span class="p">(</span>
            <span class="mi">2</span>
            <span class="o">*</span> <span class="n">n_units</span><span class="p">,</span>  <span class="c1"># the input shape of between-sentence attention layer
</span>            <span class="n">return_coefficients</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lin_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">n_units</span><span class="p">,</span> <span class="mi">1</span>  <span class="c1"># the input size of the last linear layer
</span>        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">preds</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">doc_ints</span><span class="p">):</span>
        <span class="n">sent_att_vecs_dr</span><span class="p">,</span> <span class="n">word_att_coeffs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">timeDistributed</span><span class="p">(</span>
            <span class="n">doc_ints</span>
        <span class="p">)</span>  <span class="c1"># get sentence representation
</span>        <span class="n">doc_sa</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gru</span><span class="p">(</span><span class="n">sent_att_vecs_dr</span><span class="p">)</span>
        <span class="n">doc_att_vec</span><span class="p">,</span> <span class="n">sent_att_coeffs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">doc_sa</span><span class="p">)</span>
        <span class="n">doc_att_vec_dr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">doc_att_vec</span><span class="p">)</span>
        <span class="n">doc_att_vec_dr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lin_out</span><span class="p">(</span><span class="n">doc_att_vec_dr</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">preds</span><span class="p">(</span><span class="n">doc_att_vec_dr</span><span class="p">),</span> <span class="n">word_att_coeffs</span><span class="p">,</span> <span class="n">sent_att_coeffs</span>

</code></pre></div></div> <p>While HAN is an interesting architecture, it has a major limitation. For a given sentence, the embedding is produced in isolation. This means, it ignores the other sentences. So, If redundant parts exist in the document; the model can spend the attention budget on them and neglects the others aspects.</p> <h3 id="dataset">Dataset</h3> <p>The dataset considered in this application is IMDB review dataset. It contains reviews about movies and their classes, positive or negative. The goal is to classify these reviews into positive or negative.</p> <p>Each review is converted to an array of integers that has a size of \((1, doc_size, sent_size)\). the <em>doc{_}size</em> specifies the maximum number of allowed sentences per document and <em>sent{_}size</em> the specifies the maximum number of allowed words per sentence. Smaller sentences are padded by a special padding token and smaller documents are padded with sentences containing only a special padding token. Longer documents or sentences are truncated.</p> <p>The mapping of a word to an integers is done by creating a vocabulary dictionary from the training set where each word has an integer value. The most frequent word has a value of 2. 0 and 1 are reserved for special token and out of vocabulary token.</p> <p>An example of a sentence and its mapping to an array of integers:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sentence:

"There 's a sign on The Lost Highway that says : OOV SPOILERS OOV ( but you already knew that , did n't you ? )"
</code></pre></div></div> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Corresponding array:

array([  130,    14,     6,  1991,    28,    22,  2746, 17943,    13,
         564,    85,     1,  3225,     1,    25,    26,    29,   488,
         697,    13,     3,    84,    27,    29,    45,    24,     0,
           0,     0,     0])
</code></pre></div></div> <p>We have some 0 in the end of the array as the choosen <em>sent_size</em> is 30 and the sentence needs to be padded because it contains only 26 words.</p> <h3 id="training">Training</h3> <p>The training consists of minimizing a loss function. As we are dealing with classification problem, the loss used is <em>Binary Cross Entropy (BCE)</em>:</p> \[L(\hat{y}, y) =\frac{1}{N} \sum_{i=1}^N y_i log(\hat{y}_i) + (1-y_i)log(1-\hat{y}_i)\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">HAN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># learning rate
</span><span class="n">criterion</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BCELoss</span><span class="p">()</span>
<span class="p">)</span>  <span class="c1"># Binary cross entropy from torch.nn: https://pytorch.org/docs/stable/nn.html#loss-functions
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>  <span class="c1"># Adam optimizer
</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="n">x_train</span><span class="o">=</span><span class="n">my_docs_array_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="o">=</span><span class="n">my_labels_array_train</span><span class="p">,</span>
    <span class="n">x_test</span><span class="o">=</span><span class="n">my_docs_array_test</span><span class="p">,</span>
    <span class="n">y_test</span><span class="o">=</span><span class="n">my_labels_array_test</span><span class="p">,</span>
    <span class="n">word_dict</span><span class="o">=</span><span class="n">index_to_word</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="p">):</span>

    <span class="n">train_data</span> <span class="o">=</span> <span class="nf">get_loader</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="nf">get_loader</span><span class="p">(</span><span class="n">my_docs_array_test</span><span class="p">,</span> <span class="n">my_labels_array_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="n">best_validation_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># patience
</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">with</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="sh">"</span><span class="s">batch</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">tepoch</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">tepoch</span><span class="p">):</span>
                <span class="n">tepoch</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="nb">input</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">document</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>    <span class="c1"># compute the loss
</span>                <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">0.5</span>
                <span class="p">)</span>  <span class="c1"># Clipping to prevent exploding gradient
</span>                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

                <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
                <span class="n">accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="n">label</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">batch_size</span>
                <span class="n">accuracies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
                <span class="n">tepoch</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span>
                    <span class="n">loss</span><span class="o">=</span><span class="nf">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span>
                    <span class="n">accuracy</span><span class="o">=</span><span class="mf">100.0</span> <span class="o">*</span> <span class="nf">sum</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">accuracies</span><span class="p">),</span>
                <span class="p">)</span>

        <span class="n">train_acc</span> <span class="o">=</span> <span class="nf">evaluate_accuracy</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="nf">evaluate_accuracy</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">===&gt; Epoch {} Complete: Avg. Loss: {:.4f}, Validation Accuracy: {:3.2f}%</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">test_acc</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">test_acc</span> <span class="o">&gt;=</span> <span class="n">best_validation_acc</span><span class="p">:</span>
            <span class="n">best_validation_acc</span> <span class="o">=</span> <span class="n">test_acc</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Validation accuracy improved, saving model...</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sh">"</span><span class="s">./best_model.pt</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nf">print</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="n">my_patience</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span>
                    <span class="sh">"</span><span class="s">Validation accuracy did not improve for {} epochs, stopping training...</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                        <span class="n">my_patience</span>
                    <span class="p">)</span>
                <span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading best checkpoint...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">./best_model.pt</span><span class="sh">"</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">done.</span><span class="sh">"</span><span class="p">)</span>


<span class="nf">train</span><span class="p">()</span>
</code></pre></div></div> <h3 id="analysis">Analysis</h3> <p>Using the mean of the hidden state of GRU instead attention will lead to treating the hidden state in a sequence equally. This lead to the same contribution of irrelevant elements to the prediction with the same importance as relevant element. Which can be subject to poor performances. So, the advantage of the attention mechanism is to have weights releaving the importance of each hidden state; and so each element, in the sequence (words in case of sentence, and sentences in case of a document).</p> <p>The following plots, shows the attention coefficient per words and per sentences of positive and negative reviews from the test set. The more the sentence or the word is red, the more the attention of this element is high. <em>&lt;|OOV|&gt;</em> means Out Of Vocabulary, is a token to replace words that are not in the training vocabulary.</p> <p>Example of positive review :</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/han_blog/last_review_coeff-480.webp 480w,/assets/blog/han_blog/last_review_coeff-800.webp 800w,/assets/blog/han_blog/last_review_coeff-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/blog/han_blog/last_review_coeff.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Example of Negative review :</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/han_blog/neg_review_coeff-480.webp 480w,/assets/blog/han_blog/neg_review_coeff-800.webp 800w,/assets/blog/han_blog/neg_review_coeff-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/blog/han_blog/neg_review_coeff.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>From the plots, we can see that words and sentence that relevant and more linked to a positive or negative reviews have a high attention coefficient regarding the other words. For example in the positive review, we have words such as “Brilliant”, “Master Piece”, “Great” which are positive and have a high attention. Same remark for sentences such as “:) First of all, Mulholland Drive is downright brilliant.” or “A masterpiece”. In the negative review we have words or sentences with a negative tonality have a high attention; such as “confused” “suspenful”.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Rida Lefdali. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>